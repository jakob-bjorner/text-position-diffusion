{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import fire\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import mup\n",
    "import numpy as np\n",
    "import lib.ddp\n",
    "import lib.datasets\n",
    "import lib.decay_to_init\n",
    "import lib.ema\n",
    "import lib.models\n",
    "import lib.ops\n",
    "import lib.utils\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed.optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import argparse\n",
    "import collections\n",
    "import functools\n",
    "import types\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args, title=None):\n",
    "    if isinstance(args, argparse.Namespace):\n",
    "        args = vars(args)\n",
    "    if title:\n",
    "        print(f'{title} args:')\n",
    "    else:\n",
    "        print('Args:')\n",
    "    for k, v in sorted(args.items()):\n",
    "        print(f'\\t{k}: {v}')\n",
    "\n",
    "def print_model(model):\n",
    "    print('Parameters:')\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"\\t{name}: {list(param.shape)}, std {param.std()}\")\n",
    "        if len(list(param.shape)) == 0:\n",
    "            total_params += 1\n",
    "        else:\n",
    "            total_params += functools.reduce(\n",
    "                (lambda x,y: x*y), list(param.shape))\n",
    "    print(f'Total parameters: {total_params:,}')\n",
    "\n",
    "def print_tensor(label, tensor):\n",
    "    \"\"\"Print a tensor with a given label.\"\"\"\n",
    "    torch.set_printoptions(precision=3, linewidth=119, sci_mode=False)\n",
    "    print(f'{label}:')\n",
    "    for line in str(tensor).splitlines():\n",
    "        print(f\"\\t{line}\")\n",
    "    torch.set_printoptions(profile='default')\n",
    "\n",
    "def print_row(*row, colwidth=10):\n",
    "    \"\"\"Print a row of values.\"\"\"\n",
    "    def format_val(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.item()\n",
    "        if np.issubdtype(type(x), np.floating):\n",
    "            x = \"{:.4f}\".format(x)\n",
    "        return str(x).ljust(colwidth)[:colwidth]\n",
    "    print(\"  \".join([format_val(x) for x in row]))\n",
    "\n",
    "def train_loop(\n",
    "    forward,\n",
    "    opt,\n",
    "    steps,\n",
    "    names=[],\n",
    "    hook=None,\n",
    "    print_freq=1000,\n",
    "    first_step=0,\n",
    "    lr_warmup_steps=0,\n",
    "    lr_decay=False,\n",
    "    amp_grad_scaler=True,\n",
    "    grad_accum_steps=1,\n",
    "    ddp_models=[],\n",
    "    clip_params=[],\n",
    "    clip_quantile=0.95\n",
    "    ):\n",
    "\n",
    "    def lr_fn(step):\n",
    "        if (step - first_step) < 10:\n",
    "            # Zero LR for the first 10 steps to warm up Adam\n",
    "            return 0.\n",
    "        elif step < lr_warmup_steps:\n",
    "            return float(step) / lr_warmup_steps\n",
    "        elif lr_decay:\n",
    "            # Linear to zero\n",
    "            return 1. - (float(step-lr_warmup_steps) / (1e-8+steps-lr_warmup_steps))\n",
    "        else:\n",
    "            return 1.\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(opt, lr_fn)\n",
    "\n",
    "    print_row('step', 'step time', 'loss', *names, 'grad norm', 'mem')\n",
    "    histories = collections.defaultdict(lambda: [])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp_grad_scaler)\n",
    "    start_time = time.time()\n",
    "    prev_grad_norms = torch.full([1000], 1e8, device='cuda')\n",
    "    for step in range(steps):\n",
    "\n",
    "        if step < first_step:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                scheduler.step()\n",
    "            continue\n",
    "\n",
    "        for accum_step in range(grad_accum_steps):\n",
    "\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                if accum_step < grad_accum_steps - 1:\n",
    "                    for m in ddp_models:\n",
    "                        stack.enter_context(m.no_sync())\n",
    "\n",
    "                forward_vals = forward(\n",
    "                    step,\n",
    "                    (accum_step * lib.ddp.world_size()) + lib.ddp.rank(),\n",
    "                    lib.ddp.world_size() * grad_accum_steps\n",
    "                )\n",
    "                if not isinstance(forward_vals, tuple):\n",
    "                    forward_vals = (forward_vals,)\n",
    "\n",
    "                scaled_loss = forward_vals[0] / grad_accum_steps\n",
    "                scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            histories['loss'].append(forward_vals[0].item())\n",
    "            for name, val in zip(names, forward_vals[1:]):\n",
    "                histories[name].append(val.item())\n",
    "\n",
    "            del forward_vals\n",
    "\n",
    "        scaler.unscale_(opt)\n",
    "        with torch.no_grad():\n",
    "            threshold = torch.quantile(prev_grad_norms, clip_quantile)\n",
    "            grad_norm = nn.utils.clip_grad_norm_(clip_params, threshold)\n",
    "            histories['grad_norm'].append(grad_norm.item())\n",
    "            prev_grad_norms[step % len(prev_grad_norms)] = grad_norm\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            scheduler.step()\n",
    "\n",
    "        if (step==0) or (step % print_freq == (print_freq - 1)):\n",
    "            means = {\n",
    "                name: lib.ddp.reduce_mean(np.mean(histories[name]))\n",
    "                for name in histories.keys()\n",
    "            }\n",
    "            means['step_time'] = (time.time() - start_time) / max(step - first_step, 1)\n",
    "            print_row(\n",
    "                step,\n",
    "                means['step_time'],\n",
    "                means['loss'],\n",
    "                *[means[name] for name in names],\n",
    "                means['grad_norm'],\n",
    "                torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            )\n",
    "            histories.clear()\n",
    "\n",
    "        if hook is not None:\n",
    "            hook(step)\n",
    "\n",
    "        if step == 0:\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args:\n",
      "\tauto_resume: False\n",
      "\tbatch_size: 256\n",
      "\tbeta1: 0.9\n",
      "\tbeta2: 0.99\n",
      "\tbias_warmup_steps: 5000\n",
      "\tclip_quantile: 0.95\n",
      "\tdataset: wikitext2\n",
      "\tdecay_to_init: 0.0\n",
      "\tdim: 2048\n",
      "\tema: 0.0\n",
      "\tembed_dim: 16\n",
      "\tfinal_val_steps: 5\n",
      "\tfirst_step: 0\n",
      "\tgamma_0: -3.0\n",
      "\tgamma_1: 6.0\n",
      "\tgrad_accum_steps: 1\n",
      "\thook_freq: 10000\n",
      "\tlr: 0.0014\n",
      "\tlr_decay: True\n",
      "\tlr_warmup_steps: 2500\n",
      "\tn_blocks: 24\n",
      "\tn_heads: 32\n",
      "\tn_short_seqs: 2\n",
      "\tprint_freq: 1000\n",
      "\treconst_bs_ema: 0.997\n",
      "\treconst_weight: 1.0\n",
      "\tsave_weights: True\n",
      "\tselfcond: True\n",
      "\tseq_len: 1024\n",
      "\tsteps: 0\n",
      "\tval_batch_size: 1\n",
      "\tval_steps: 100\n",
      "\tweight_decay: 4e-05\n",
      "\tweights_path: /srv/flash2/jbjorner3/plaid-model/plaid1b_weights\n",
      "Words per token (wikitext2): 0.8450820067036953\n",
      "Number of iterations for (wikitext2): 284\n",
      "vocab_size: 32768\n",
      "noise_schedule:\n",
      "Parameters:\n",
      "\tW1: [1024, 1], std 0.9747692346572876\n",
      "\tb1: [1024], std 0.9946915507316589\n",
      "\tW2: [1, 1024], std 0.9626893401145935\n",
      "Total parameters: 3,072\n",
      "gamma_bounds:\n",
      "Parameters:\n",
      "\tgamma_0: [], std nan\n",
      "\tgamma_1: [], std nan\n",
      "Total parameters: 2\n",
      "embedding_matrix:\n",
      "Parameters:\n",
      "\tmatrix: [32768, 16], std 0.2500001788139343\n",
      "Total parameters: 524,288\n",
      "model:\n",
      "Parameters:\n",
      "\tinput_linear.weight: [2048, 16], std 0.14448264241218567\n",
      "\tselfcond_linear.weight: [2048, 16], std 0.0\n",
      "\tgamma_linear.weight: [2048, 64], std 0.0\n",
      "\tblocks.0.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.0.attn_qkv.weight: [6144, 2048], std 0.01275674719363451\n",
      "\tblocks.0.attn_out.weight: [2048, 2048], std 0.012755543924868107\n",
      "\tblocks.0.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.0.mlp.fc1.weight: [8192, 2048], std 0.012759482488036156\n",
      "\tblocks.0.mlp.fc2.weight: [2048, 8192], std 0.006380284205079079\n",
      "\tblocks.1.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.1.attn_qkv.weight: [6144, 2048], std 0.012761707417666912\n",
      "\tblocks.1.attn_out.weight: [2048, 2048], std 0.01276161428540945\n",
      "\tblocks.1.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.1.mlp.fc1.weight: [8192, 2048], std 0.012759249657392502\n",
      "\tblocks.1.mlp.fc2.weight: [2048, 8192], std 0.00637896079570055\n",
      "\tblocks.2.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.2.attn_qkv.weight: [6144, 2048], std 0.012757835909724236\n",
      "\tblocks.2.attn_out.weight: [2048, 2048], std 0.012762263417243958\n",
      "\tblocks.2.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.2.mlp.fc1.weight: [8192, 2048], std 0.012755483388900757\n",
      "\tblocks.2.mlp.fc2.weight: [2048, 8192], std 0.006378557998687029\n",
      "\tblocks.3.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.3.attn_qkv.weight: [6144, 2048], std 0.012756915763020515\n",
      "\tblocks.3.attn_out.weight: [2048, 2048], std 0.012756383046507835\n",
      "\tblocks.3.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.3.mlp.fc1.weight: [8192, 2048], std 0.01275536697357893\n",
      "\tblocks.3.mlp.fc2.weight: [2048, 8192], std 0.006378411315381527\n",
      "\tblocks.4.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.4.attn_qkv.weight: [6144, 2048], std 0.012758615426719189\n",
      "\tblocks.4.attn_out.weight: [2048, 2048], std 0.012757617980241776\n",
      "\tblocks.4.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.4.mlp.fc1.weight: [8192, 2048], std 0.012755930423736572\n",
      "\tblocks.4.mlp.fc2.weight: [2048, 8192], std 0.006378028076142073\n",
      "\tblocks.5.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.5.attn_qkv.weight: [6144, 2048], std 0.012756303884088993\n",
      "\tblocks.5.attn_out.weight: [2048, 2048], std 0.01275995746254921\n",
      "\tblocks.5.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.5.mlp.fc1.weight: [8192, 2048], std 0.012758835218846798\n",
      "\tblocks.5.mlp.fc2.weight: [2048, 8192], std 0.00637848163023591\n",
      "\tblocks.6.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.6.attn_qkv.weight: [6144, 2048], std 0.012756604701280594\n",
      "\tblocks.6.attn_out.weight: [2048, 2048], std 0.012758919037878513\n",
      "\tblocks.6.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.6.mlp.fc1.weight: [8192, 2048], std 0.012756417505443096\n",
      "\tblocks.6.mlp.fc2.weight: [2048, 8192], std 0.006378600839525461\n",
      "\tblocks.7.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.7.attn_qkv.weight: [6144, 2048], std 0.012756704352796078\n",
      "\tblocks.7.attn_out.weight: [2048, 2048], std 0.01275756023824215\n",
      "\tblocks.7.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.7.mlp.fc1.weight: [8192, 2048], std 0.012758525088429451\n",
      "\tblocks.7.mlp.fc2.weight: [2048, 8192], std 0.006378920748829842\n",
      "\tblocks.8.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.8.attn_qkv.weight: [6144, 2048], std 0.012757769785821438\n",
      "\tblocks.8.attn_out.weight: [2048, 2048], std 0.012761712074279785\n",
      "\tblocks.8.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.8.mlp.fc1.weight: [8192, 2048], std 0.01275827270001173\n",
      "\tblocks.8.mlp.fc2.weight: [2048, 8192], std 0.0063789584673941135\n",
      "\tblocks.9.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.9.attn_qkv.weight: [6144, 2048], std 0.012755547650158405\n",
      "\tblocks.9.attn_out.weight: [2048, 2048], std 0.012752043083310127\n",
      "\tblocks.9.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.9.mlp.fc1.weight: [8192, 2048], std 0.01275622844696045\n",
      "\tblocks.9.mlp.fc2.weight: [2048, 8192], std 0.006378734949976206\n",
      "\tblocks.10.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.10.attn_qkv.weight: [6144, 2048], std 0.012756950221955776\n",
      "\tblocks.10.attn_out.weight: [2048, 2048], std 0.01275813952088356\n",
      "\tblocks.10.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.10.mlp.fc1.weight: [8192, 2048], std 0.012759152799844742\n",
      "\tblocks.10.mlp.fc2.weight: [2048, 8192], std 0.006378973368555307\n",
      "\tblocks.11.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.11.attn_qkv.weight: [6144, 2048], std 0.012756540440022945\n",
      "\tblocks.11.attn_out.weight: [2048, 2048], std 0.012752784416079521\n",
      "\tblocks.11.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.11.mlp.fc1.weight: [8192, 2048], std 0.012756779789924622\n",
      "\tblocks.11.mlp.fc2.weight: [2048, 8192], std 0.0063790190033614635\n",
      "\tblocks.12.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.12.attn_qkv.weight: [6144, 2048], std 0.012757251970469952\n",
      "\tblocks.12.attn_out.weight: [2048, 2048], std 0.012759038247168064\n",
      "\tblocks.12.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.12.mlp.fc1.weight: [8192, 2048], std 0.012757805176079273\n",
      "\tblocks.12.mlp.fc2.weight: [2048, 8192], std 0.006379281170666218\n",
      "\tblocks.13.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.13.attn_qkv.weight: [6144, 2048], std 0.01275841984897852\n",
      "\tblocks.13.attn_out.weight: [2048, 2048], std 0.012756249867379665\n",
      "\tblocks.13.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.13.mlp.fc1.weight: [8192, 2048], std 0.012755787931382656\n",
      "\tblocks.13.mlp.fc2.weight: [2048, 8192], std 0.00637961458414793\n",
      "\tblocks.14.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.14.attn_qkv.weight: [6144, 2048], std 0.01276045385748148\n",
      "\tblocks.14.attn_out.weight: [2048, 2048], std 0.012756947427988052\n",
      "\tblocks.14.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.14.mlp.fc1.weight: [8192, 2048], std 0.012758483178913593\n",
      "\tblocks.14.mlp.fc2.weight: [2048, 8192], std 0.0063790250569581985\n",
      "\tblocks.15.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.15.attn_qkv.weight: [6144, 2048], std 0.012760347686707973\n",
      "\tblocks.15.attn_out.weight: [2048, 2048], std 0.012763267382979393\n",
      "\tblocks.15.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.15.mlp.fc1.weight: [8192, 2048], std 0.012758147902786732\n",
      "\tblocks.15.mlp.fc2.weight: [2048, 8192], std 0.006378252990543842\n",
      "\tblocks.16.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.16.attn_qkv.weight: [6144, 2048], std 0.012755204923450947\n",
      "\tblocks.16.attn_out.weight: [2048, 2048], std 0.012752488255500793\n",
      "\tblocks.16.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.16.mlp.fc1.weight: [8192, 2048], std 0.012758498080074787\n",
      "\tblocks.16.mlp.fc2.weight: [2048, 8192], std 0.006379011087119579\n",
      "\tblocks.17.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.17.attn_qkv.weight: [6144, 2048], std 0.01275757234543562\n",
      "\tblocks.17.attn_out.weight: [2048, 2048], std 0.01275843195617199\n",
      "\tblocks.17.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.17.mlp.fc1.weight: [8192, 2048], std 0.012757360003888607\n",
      "\tblocks.17.mlp.fc2.weight: [2048, 8192], std 0.00637789024040103\n",
      "\tblocks.18.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.18.attn_qkv.weight: [6144, 2048], std 0.01275396253913641\n",
      "\tblocks.18.attn_out.weight: [2048, 2048], std 0.012755225412547588\n",
      "\tblocks.18.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.18.mlp.fc1.weight: [8192, 2048], std 0.012758529745042324\n",
      "\tblocks.18.mlp.fc2.weight: [2048, 8192], std 0.0063790008425712585\n",
      "\tblocks.19.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.19.attn_qkv.weight: [6144, 2048], std 0.012756207026541233\n",
      "\tblocks.19.attn_out.weight: [2048, 2048], std 0.012761261314153671\n",
      "\tblocks.19.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.19.mlp.fc1.weight: [8192, 2048], std 0.012759043835103512\n",
      "\tblocks.19.mlp.fc2.weight: [2048, 8192], std 0.0063794697634875774\n",
      "\tblocks.20.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.20.attn_qkv.weight: [6144, 2048], std 0.012758936733007431\n",
      "\tblocks.20.attn_out.weight: [2048, 2048], std 0.01276108343154192\n",
      "\tblocks.20.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.20.mlp.fc1.weight: [8192, 2048], std 0.012759286910295486\n",
      "\tblocks.20.mlp.fc2.weight: [2048, 8192], std 0.006379239726811647\n",
      "\tblocks.21.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.21.attn_qkv.weight: [6144, 2048], std 0.012757256627082825\n",
      "\tblocks.21.attn_out.weight: [2048, 2048], std 0.012755854986608028\n",
      "\tblocks.21.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.21.mlp.fc1.weight: [8192, 2048], std 0.012757796794176102\n",
      "\tblocks.21.mlp.fc2.weight: [2048, 8192], std 0.006377896293997765\n",
      "\tblocks.22.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.22.attn_qkv.weight: [6144, 2048], std 0.012759336270391941\n",
      "\tblocks.22.attn_out.weight: [2048, 2048], std 0.012758489698171616\n",
      "\tblocks.22.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.22.mlp.fc1.weight: [8192, 2048], std 0.012759769335389137\n",
      "\tblocks.22.mlp.fc2.weight: [2048, 8192], std 0.006379160098731518\n",
      "\tblocks.23.rmsnorm1.weight: [2048], std 0.0\n",
      "\tblocks.23.attn_qkv.weight: [6144, 2048], std 0.012758013792335987\n",
      "\tblocks.23.attn_out.weight: [2048, 2048], std 0.012761036865413189\n",
      "\tblocks.23.rmsnorm2.weight: [2048], std 0.0\n",
      "\tblocks.23.mlp.fc1.weight: [8192, 2048], std 0.012756182812154293\n",
      "\tblocks.23.mlp.fc2.weight: [2048, 8192], std 0.00637810118496418\n",
      "\toutput_norm.weight: [2048], std 0.0\n",
      "\toutput_linear.weight: [32768, 2048], std 0.0\n",
      "\toutput_linear.bias: [32768], std 0.0\n",
      "Total parameters: 1,275,398,144\n"
     ]
    }
   ],
   "source": [
    "# python train.py --weights_path=/path/to/plaid1b_weights --dim=2048 --n_blocks=24 --n_heads=32 --seq_len=1024 --dataset=wikitext103\n",
    "args = {\n",
    "    'weights_path': '/srv/flash2/jbjorner3/plaid-model/plaid1b_weights',\n",
    "    'dim': 2048,\n",
    "    'n_blocks': 24,\n",
    "    'n_heads': 32,\n",
    "    'seq_len': 1024,\n",
    "    \n",
    "    'dataset': 'wikitext2',\n",
    "    'val_batch_size': 1,\n",
    "    \"steps\": 0,\n",
    "    \"final_val_steps\": 5,\n",
    "}\n",
    "args = lib.utils.AttributeDict(args)\n",
    "args.setdefault('batch_size', 256)\n",
    "args.setdefault('dataset', 'openwebtext2')\n",
    "args.setdefault('grad_accum_steps', 1)\n",
    "args.setdefault('hook_freq', 10000)\n",
    "args.setdefault('lr', 1.4e-3)\n",
    "args.setdefault('lr_warmup_steps', 2500)\n",
    "args.setdefault('bias_warmup_steps', 5000)\n",
    "args.setdefault('lr_decay', True)\n",
    "args.setdefault('print_freq', 1000)\n",
    "args.setdefault('save_weights', True)\n",
    "args.setdefault('steps', 92000)\n",
    "args.setdefault('weights_path', None)\n",
    "args.setdefault('reconst_weight', 1.0)\n",
    "args.setdefault('dim', 384)\n",
    "args.setdefault('n_blocks', 16)\n",
    "args.setdefault('n_heads', 6)\n",
    "args.setdefault('gamma_0', -3.)\n",
    "args.setdefault('gamma_1', 6.)\n",
    "args.setdefault('embed_dim', 16)\n",
    "args.setdefault('seq_len', 256)\n",
    "args.setdefault('val_steps', 100)\n",
    "args.setdefault('val_batch_size', 64)\n",
    "args.setdefault('weight_decay', 4e-5)\n",
    "args.setdefault('first_step', 0)\n",
    "args.setdefault('auto_resume', False)\n",
    "args.setdefault('decay_to_init', 0.)\n",
    "args.setdefault('ema', 0.)\n",
    "args.setdefault('beta1', 0.9)\n",
    "args.setdefault('beta2', 0.99)\n",
    "args.setdefault('selfcond', True)\n",
    "args.setdefault('n_short_seqs', 2)\n",
    "args.setdefault('clip_quantile', 0.95)\n",
    "args.setdefault('reconst_bs_ema', 0.997)\n",
    "args.setdefault('final_val_steps', 300000000)\n",
    "\n",
    "lib.utils.print_args(args)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# Lots of annoying big/small numbers throughout this code, so we'll do\n",
    "# everything in fp64 by default and explicitly switch to fp32/bf16 where\n",
    "# appropriate.\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "dataset = lib.datasets.REGISTRY[args.dataset](\n",
    "    args.batch_size, args.val_batch_size, args.seq_len\n",
    ")\n",
    "(train_iterator,val_iterator,test_iterator), (word2idx, idx2word) = dataset\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "def create_modules(dim, n_heads):\n",
    "    return {\n",
    "        'noise_schedule': lib.models.NoiseSchedule().float(),\n",
    "        'gamma_bounds': lib.models.GammaBounds(args.gamma_0, args.gamma_1).float(),\n",
    "        'embedding_matrix': lib.models.EmbeddingMatrix(vocab_size, args.embed_dim).float(),\n",
    "        'model': lib.models.DiffusionModel(dim, args.embed_dim, args.n_blocks, n_heads, vocab_size).float()\n",
    "    }\n",
    "modules = create_modules(args.dim, args.n_heads)\n",
    "base_modules = create_modules(256, 4)\n",
    "delta_modules = create_modules(128, 2)\n",
    "for key in modules:\n",
    "    main, base, delta = modules[key], base_modules[key], delta_modules[key]\n",
    "    mup.set_base_shapes(main, base, delta=delta)\n",
    "    main.cuda()\n",
    "    print(key+':')\n",
    "    lib.utils.print_model(main)\n",
    "\n",
    "def load_weights(weights_path):\n",
    "    print(f'Loading weights from {weights_path}')\n",
    "    for name, module in modules.items():\n",
    "        module.load_state_dict(torch.load(\n",
    "            os.path.join(weights_path, f'{name}.pt'),\n",
    "            map_location=torch.device('cuda')\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from /srv/flash2/jbjorner3/plaid-model/plaid1b_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from step 0\n"
     ]
    }
   ],
   "source": [
    "if args.auto_resume:\n",
    "    assert(args.save_weights)\n",
    "\n",
    "first_step = args.first_step\n",
    "if args.auto_resume and os.path.exists('model.pt'):\n",
    "        load_weights('.')\n",
    "        with open('step', 'r') as f:\n",
    "            first_step = int(f.read()) + 1\n",
    "elif args.weights_path is not None:\n",
    "    load_weights(args.weights_path)\n",
    "\n",
    "print(f'Starting from step {first_step}')\n",
    "\n",
    "ddp_modules = {\n",
    "    # name: DDP(module, broadcast_buffers=False,\n",
    "    #     find_unused_parameters=True,\n",
    "    #     gradient_as_bucket_view=True\n",
    "    # )\n",
    "    name: module\n",
    "    for name, module in modules.items()\n",
    "}\n",
    "\n",
    "# print('DDP initialized')\n",
    "\n",
    "emas = {\n",
    "    name: lib.ema.EMA(module, args.ema)\n",
    "    for name, module in modules.items()\n",
    "}\n",
    "\n",
    "decay_to_init = {\n",
    "    name: lib.decay_to_init.DecayToInit(module, args.decay_to_init)\n",
    "    for name, module in modules.items()\n",
    "}\n",
    "\n",
    "loss_ema_bias     = torch.tensor(1e-8)\n",
    "reconst_ema       = torch.tensor(1e-8)\n",
    "diffusion_ema     = torch.tensor(1e-8)\n",
    "reconst_sqr_ema   = torch.tensor(1e-8)\n",
    "diffusion_sqr_ema = torch.tensor(1e-8)\n",
    "reconst_bs_cache  = {}\n",
    "def forward(step=None, accum_step=None, accum_total=None, x_eval=None):\n",
    "    \"\"\"\n",
    "    Train mode: step, accum_step, accum_total\n",
    "    Eval mode: x_eval\n",
    "    \"\"\"\n",
    "    global reconst_ema, diffusion_ema, reconst_sqr_ema, diffusion_sqr_ema\n",
    "\n",
    "    train_mode = (x_eval is None)\n",
    "    if train_mode:\n",
    "        x = next(train_iterator)\n",
    "        batch_size = x.shape[0] * accum_total\n",
    "        if step not in reconst_bs_cache:\n",
    "            # Synchronize EMA vars\n",
    "            reconst_ema       = lib.ddp.reduce_mean(reconst_ema)\n",
    "            reconst_sqr_ema   = lib.ddp.reduce_mean(reconst_sqr_ema)\n",
    "            diffusion_ema     = lib.ddp.reduce_mean(diffusion_ema)\n",
    "            diffusion_sqr_ema = lib.ddp.reduce_mean(diffusion_sqr_ema)\n",
    "            # Compute reconst_bs\n",
    "            b = 1 / loss_ema_bias # Bias correction factor\n",
    "            reconst_std   = (b*reconst_sqr_ema   - (b*reconst_ema)**2).clamp(min=0).sqrt()\n",
    "            diffusion_std = (b*diffusion_sqr_ema - (b*diffusion_ema)**2).clamp(min=0).sqrt()\n",
    "            reconst_bs = batch_size * (reconst_std / (1e-8 + reconst_std + diffusion_std))\n",
    "            reconst_bs = int(reconst_bs.round().clamp(1, batch_size-1))\n",
    "            reconst_bs_cache[step] = reconst_bs\n",
    "        reconst_bs = reconst_bs_cache[step]\n",
    "        avg_reconst_bs = float(reconst_bs)\n",
    "    else:\n",
    "        x = x_eval\n",
    "        batch_size = x.shape[0]\n",
    "        reconst_bs = (batch_size // 8)\n",
    "        reconst_bs += int(np.random.binomial(1, (batch_size % 8) / 8.))\n",
    "        avg_reconst_bs = batch_size / 8.\n",
    "\n",
    "    embedding_matrix = ddp_modules['embedding_matrix']()\n",
    "\n",
    "    selfcond_mask = torch.zeros([batch_size], device='cuda')\n",
    "    avg_selfcond_mask = 0.\n",
    "    if args.selfcond:\n",
    "        if train_mode:\n",
    "            offset = int(np.random.randint(4))\n",
    "            selfcond_mask[offset::4].add_(1)\n",
    "            avg_selfcond_mask = 0.25\n",
    "        else:\n",
    "            selfcond_mask.add_(1)\n",
    "            avg_selfcond_mask = 1.\n",
    "\n",
    "    t = torch.empty([batch_size], device='cuda')\n",
    "    # First entries of t are used for reconst_loss below\n",
    "    t[:reconst_bs] = 0\n",
    "    # Low-discrepancy sampler for the remaining entries of t\n",
    "    t[reconst_bs:] = torch.arange(\n",
    "        batch_size - reconst_bs, device='cuda')\n",
    "    if train_mode:\n",
    "        t[reconst_bs:] += float(np.random.RandomState(step).uniform())\n",
    "    else:\n",
    "        t[reconst_bs:] += float(np.random.uniform())\n",
    "    t[reconst_bs:] /= batch_size - reconst_bs\n",
    "    t.requires_grad = True\n",
    "\n",
    "    if train_mode:\n",
    "        batch_size //= accum_total\n",
    "        selfcond_mask = selfcond_mask.chunk(accum_total)[accum_step]\n",
    "        t = t.chunk(accum_total)[accum_step]\n",
    "        reconst_bs = int(t.eq(0).sum())\n",
    "        avg_reconst_bs /= accum_total\n",
    "\n",
    "    selfcond_idx = selfcond_mask.nonzero()[:,0]\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Don't propagate grads for the first reconst_bs entries of t\n",
    "        gamma = torch.cat([\n",
    "            ddp_modules['noise_schedule'](t[:reconst_bs]).detach(),\n",
    "            ddp_modules['noise_schedule'](t[reconst_bs:])\n",
    "        ])\n",
    "        gamma_prime = autograd.grad(gamma.sum(), [t], create_graph=True)[0]\n",
    "    # Edits gradients so that the noise schedule minimizes\n",
    "    # E[loss^2] while the rest of the model minimizes E[loss].\n",
    "    def set_grad_hook(tensor):\n",
    "        if tensor.requires_grad:\n",
    "            def grad_hook(grad):\n",
    "                handle.remove()\n",
    "                new_grad = torch.clone(grad.detach())\n",
    "                new_grad[reconst_bs:] *= 2. * (\n",
    "                    grad_hook_loss[reconst_bs:].detach()\n",
    "                )\n",
    "                return new_grad\n",
    "            handle = tensor.register_hook(grad_hook)\n",
    "    gamma = gamma.clone()\n",
    "    set_grad_hook(gamma)\n",
    "    set_grad_hook(gamma_prime)\n",
    "    gamma_0, gamma_1 = ddp_modules['gamma_bounds']()\n",
    "    gamma = gamma_0 + (gamma_1 - gamma_0) * gamma\n",
    "    gamma_prime = (gamma_1 - gamma_0) * gamma_prime\n",
    "\n",
    "    gamma = torch.lerp(gamma, gamma.detach(), selfcond_mask)\n",
    "    gamma_prime = torch.lerp(gamma_prime, gamma_prime.detach(), selfcond_mask)\n",
    "\n",
    "    # Quantities derived from gamma, gamma_prime, gamma_1:\n",
    "    alpha_squared = torch.sigmoid(-gamma)\n",
    "    sigma_squared = torch.sigmoid(gamma)\n",
    "    alpha = alpha_squared.sqrt()\n",
    "    sigma = sigma_squared.sqrt()\n",
    "    snr_prime = -(-gamma).exp() * gamma_prime # SNR = exp(-gamma)\n",
    "    alpha_1 = torch.sigmoid(-gamma_1).sqrt()\n",
    "    sigma_1 = torch.sigmoid(gamma_1).sqrt()\n",
    "\n",
    "    # Construct z (with reparam. trick gradients)\n",
    "    x_embed = embedding_matrix[x]\n",
    "    x_embed = torch.lerp(x_embed, x_embed.detach(), selfcond_mask.float()[:,None,None])\n",
    "    z = torch.randn(\n",
    "        [x.shape[0], x.shape[1], args.embed_dim],\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    z.mul_(sigma[:,None,None])\n",
    "    z.add_(alpha[:,None,None] * x_embed)\n",
    "\n",
    "    cu_seqlens = None\n",
    "    cu_seqlens_selfcond = None\n",
    "    if train_mode:\n",
    "        accum_interval = max(accum_total // args.n_short_seqs, 1)\n",
    "        accum_offset = int(np.random.RandomState(step).randint(accum_interval))\n",
    "        accum_n = args.n_short_seqs * accum_interval // accum_total\n",
    "        if accum_step % accum_interval == accum_offset:\n",
    "            seqlens = torch.zeros([batch_size, 2], device='cuda', dtype=torch.int64)\n",
    "            seqlens[:,0] = x.shape[1]\n",
    "            positions = torch.randperm(batch_size, device='cuda')[:accum_n]\n",
    "            lens = torch.randint(1, x.shape[1], [accum_n], device='cuda')\n",
    "            seqlens[positions, 0] = lens\n",
    "            seqlens[positions, 1] = x.shape[1] - lens\n",
    "            cu_seqlens = torch.zeros([seqlens.numel()+1], dtype=torch.int32, device='cuda')\n",
    "            cu_seqlens[1:] = seqlens.view(-1).cumsum(dim=0)\n",
    "            cu_seqlens_selfcond = torch.zeros([seqlens[selfcond_idx].numel()+1], dtype=torch.int32, device='cuda')\n",
    "            cu_seqlens_selfcond[1:] = seqlens[selfcond_idx].view(-1).cumsum(dim=0)\n",
    "\n",
    "    if train_mode:\n",
    "        bias_scale = min(1., (step + 1e-8) / (args.bias_warmup_steps + 1e-8))\n",
    "    else:\n",
    "        bias_scale = 1.\n",
    "\n",
    "    # Model forward pass for self-conditioning\n",
    "    x_selfcond = torch.zeros_like(z)\n",
    "    if len(selfcond_idx) > 0:\n",
    "        with torch.no_grad():\n",
    "            z_selfcond = z[selfcond_idx]\n",
    "            gamma_selfcond = gamma[selfcond_idx]\n",
    "            logits, x_reconst = ddp_modules['model'](\n",
    "                z_selfcond, gamma_selfcond, embedding_matrix, bias_scale,\n",
    "                torch.zeros_like(z_selfcond),\n",
    "                cu_seqlens=cu_seqlens_selfcond\n",
    "            )\n",
    "            del logits\n",
    "            x_selfcond[selfcond_idx] = x_reconst\n",
    "\n",
    "    # Main model forward pass\n",
    "    with torch.enable_grad():\n",
    "        logits, x_reconst = ddp_modules['model'](\n",
    "            z, gamma, embedding_matrix, bias_scale, x_selfcond,\n",
    "            selfcond_mask=selfcond_mask,\n",
    "            cu_seqlens=cu_seqlens\n",
    "        )\n",
    "\n",
    "    # Loss terms\n",
    "    reconst_loss = lib.ops.cross_entropy(\n",
    "        logits[:reconst_bs],\n",
    "        x[:reconst_bs]\n",
    "    ).mean(dim=1).double()\n",
    "\n",
    "    alpha_1_masked = torch.lerp(alpha_1, alpha_1.detach(), selfcond_mask)[:,None,None]\n",
    "    sigma_1_masked = torch.lerp(sigma_1, sigma_1.detach(), selfcond_mask)[:,None,None]\n",
    "    prior_loss = lib.ops.gaussian_kl(\n",
    "        (alpha_1_masked * x_embed),\n",
    "        sigma_1_masked,\n",
    "        torch.tensor(0., device='cuda'),\n",
    "        torch.tensor(1., device='cuda')\n",
    "    ).sum(dim=2).mean()\n",
    "\n",
    "    diffusion_loss = (x_embed - x_reconst).pow(2)\n",
    "    diffusion_loss = diffusion_loss.mean(dim=1).double().sum(dim=1)\n",
    "    diffusion_loss = -0.5*(snr_prime * diffusion_loss)\n",
    "\n",
    "    if train_mode:\n",
    "        with torch.no_grad():\n",
    "            loss_ema_bias.lerp_(     torch.tensor(1., device='cuda'),                                                   1 - args.reconst_bs_ema)\n",
    "            reconst_ema.lerp_(       (args.reconst_weight * reconst_loss).sum()        / avg_reconst_bs,                1 - args.reconst_bs_ema)\n",
    "            reconst_sqr_ema.lerp_(   (args.reconst_weight * reconst_loss).pow(2).sum() / avg_reconst_bs,                1 - args.reconst_bs_ema)\n",
    "            diffusion_ema.lerp_(     diffusion_loss[reconst_bs:].sum()                 / (batch_size - avg_reconst_bs), 1 - args.reconst_bs_ema)\n",
    "            diffusion_sqr_ema.lerp_( diffusion_loss[reconst_bs:].pow(2).sum()          / (batch_size - avg_reconst_bs), 1 - args.reconst_bs_ema)\n",
    "\n",
    "    grad_hook_loss = diffusion_loss # Used above (weird variable scope)\n",
    "\n",
    "    loss = (args.reconst_weight * reconst_loss).sum() / avg_reconst_bs\n",
    "    loss += diffusion_loss[reconst_bs:].sum() / (batch_size - avg_reconst_bs)\n",
    "    loss += prior_loss\n",
    "\n",
    "    if args.selfcond:\n",
    "        nll = (reconst_loss * selfcond_mask[:reconst_bs]).sum() / (avg_reconst_bs * avg_selfcond_mask)\n",
    "        nll += (diffusion_loss[reconst_bs:] * selfcond_mask[reconst_bs:]).sum() / ((batch_size - avg_reconst_bs) * avg_selfcond_mask)\n",
    "        nll += prior_loss\n",
    "    else:\n",
    "        nll = reconst_loss.sum() / avg_reconst_bs\n",
    "        nll += diffusion_loss[reconst_bs:].sum() / (batch_size - avg_reconst_bs)\n",
    "        nll += prior_loss\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        nll,\n",
    "        reconst_loss.sum() / avg_reconst_bs,\n",
    "        prior_loss,\n",
    "        gamma_0,\n",
    "        gamma_1,\n",
    "        torch.tensor(reconst_bs).cuda(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(5).float(); a.requires_grad = True\n",
    "a\n",
    "with torch.no_grad():\n",
    "    with torch.enable_grad():\n",
    "        b = a * 2\n",
    "        b = b.sum()\n",
    "        print(a.grad)\n",
    "        b.backward()\n",
    "        print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "> \u001b[0;32m/tmp/ipykernel_370175/1639694739.py\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      8 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m            \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        stack.enter_context(torch.no_grad())\n",
    "        for ema in emas.values():\n",
    "            stack.enter_context(ema.enabled())\n",
    "        for val_it in val_iterator:\n",
    "            print(val_it.shape)\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            nll = forward(x_eval=val_it)[1]\n",
    "            print(nll)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05841594007103976"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1576 * 1024  # tokens in enwik8 (1613824), and 100000000 characters\n",
    "np.log2(np.exp((2.509 * 1613824) / 100000000)) # bits per character, would then be?\n",
    "\n",
    "# Final val NLL: 2.5096260238959296\n",
    "# Final val NLL (seq_len=256): 2.9215324813081054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.964625634517766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000000 / 1613824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.964625634517766"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "61.964625634517766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0559996666597347"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.268323963994549 / np.log(2) / 3.098959059651863 # bits per token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.111722729725383"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "0.84508 # words per token, convert this to the ppl for model in terms of words\n",
    "2.832978883892364 # NLL\n",
    "np.exp(2.832978883892364) # ppl\n",
    "np.exp(2.832978883892364) / 0.84508 # ppl for model in terms of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:31,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final val NLL: 2.8499310392511408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:18, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final val NLL (seq_len=256): 3.2566068461565805\n",
      "[]\n",
      "2.8499310392511408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.final_val_steps = 284 # 1576\n",
    "def compute_nll(data_iterator, steps, seq_len=args.seq_len):\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        for ema in emas.values():\n",
    "            stack.enter_context(ema.enabled())\n",
    "        stack.enter_context(torch.no_grad())\n",
    "        total_nll = 0.\n",
    "        total_tokens = 0\n",
    "        for i, X in tqdm.tqdm(enumerate(data_iterator)):\n",
    "            X = X.cuda()[:,:seq_len]\n",
    "            nll = forward(x_eval=X)[1]\n",
    "            total_nll += (nll.item() * X.numel())\n",
    "            total_tokens += X.numel()\n",
    "            if i == steps:\n",
    "                break\n",
    "    return lib.ddp.reduce_mean(total_nll / total_tokens).item()\n",
    "\n",
    "all_val_nlls = []\n",
    "def hook(step):\n",
    "    for decay in decay_to_init.values():\n",
    "        decay.step(step, args.steps)\n",
    "\n",
    "    for ema in emas.values():\n",
    "        ema.step()\n",
    "\n",
    "    if step % args.hook_freq == (args.hook_freq - 1):\n",
    "        val_nll = compute_nll(val_iterator, args.val_steps)\n",
    "        print(f'NLL (val, seq_len={args.seq_len}): {val_nll}')\n",
    "        all_val_nlls.append(val_nll)\n",
    "        if args.seq_len != 256:\n",
    "            val_nll_256 = compute_nll(val_iterator, args.val_steps, seq_len=256)\n",
    "            print(f'NLL (val, seq_len=256): {val_nll_256}')\n",
    "\n",
    "        if lib.ddp.rank() == 0:\n",
    "            # Save weights\n",
    "            if args.save_weights:\n",
    "                for name in modules:\n",
    "                    with emas[name].enabled():\n",
    "                        torch.save(modules[name].state_dict(), f'{name}.pt')\n",
    "                with open('step', 'w') as f:\n",
    "                    f.write(str(step))\n",
    "                print('Saved weights!')\n",
    "\n",
    "            # Save gamma plot\n",
    "            t = torch.linspace(0., 1., 1024).cuda()\n",
    "            gamma = modules['noise_schedule'](t)\n",
    "            plt.clf()\n",
    "            plt.plot(t.detach().cpu().numpy(), gamma.detach().cpu().numpy())\n",
    "            plt.savefig(f'gamma_{step}.jpg')\n",
    "\n",
    "print('Starting train loop...')\n",
    "# lib.utils.train_loop(\n",
    "#     forward,\n",
    "#     opt,\n",
    "#     args.steps,\n",
    "#     names=['nll','reconst','prior','gamma_0','gamma_1','reconst_bs'],\n",
    "#     hook=hook,\n",
    "#     print_freq=args.print_freq,\n",
    "#     lr_warmup_steps=args.lr_warmup_steps,\n",
    "#     lr_decay=args.lr_decay,\n",
    "#     amp_grad_scaler=False,\n",
    "#     grad_accum_steps=args.grad_accum_steps,\n",
    "#     ddp_models=ddp_modules.values(),\n",
    "#     first_step=first_step,\n",
    "#     clip_params=[\n",
    "#         param\n",
    "#         for module in modules.values()\n",
    "#         for param in module.parameters()\n",
    "#     ],\n",
    "#     clip_quantile=args.clip_quantile,\n",
    "# )\n",
    "\n",
    "final_val_nll = compute_nll(val_iterator, args.final_val_steps)\n",
    "print('Final val NLL:', final_val_nll)\n",
    "if args.seq_len != 256:\n",
    "    final_val_nll_256 = compute_nll(val_iterator, args.final_val_steps, seq_len=256)\n",
    "    print('Final val NLL (seq_len=256):', final_val_nll_256)\n",
    "\n",
    "print(all_val_nlls)\n",
    "print(final_val_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.96129684655642"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.2566068461565805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaid-v2-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
